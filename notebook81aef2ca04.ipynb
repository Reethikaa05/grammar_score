{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Grammar_Scoring_Engine.ipynb\n\n# 1. Environment Setup\n!pip install librosa transformers language-tool-python tensorflow matplotlib\n\n# 2. Data Loading\nimport pandas as pd\nimport numpy as np\nimport librosa\nfrom transformers import pipeline\nimport language_tool_python\n\ntrain_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\n\n#3. Advanced Audio Processing\ndef extract_audio_features(file_path):\n    y, sr = librosa.load(file_path, sr=16000)\n    features = {\n        'mfcc': librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40),\n        'chroma': librosa.feature.chroma_stft(y=y, sr=sr),\n        'contrast': librosa.feature.spectral_contrast(y=y, sr=sr)\n    }\n    return {k: np.mean(v, axis=1) for k, v in features.items()}\n\n# 4. Whisper Transcription\nasr_pipe = pipeline(\"automatic-speech-recognition\", \n                   model=\"openai/whisper-medium\",\n                   device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef transcribe_audio(file_path):\n    return asr_pipe(file_path)[\"text\"]\n\n# 5. Grammar Analysis\ntool = language_tool_python.LanguageTool('en-US')\n\ndef analyze_grammar(text):\n    matches = tool.check(text)\n    return {\n        'error_count': len(matches),\n        'error_types': {type(m.ruleId) for m in matches}\n    }\n\n# 6. Feature Fusion\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n# Combine audio features, text statistics, and grammar metrics\npreprocessor = ColumnTransformer([\n    ('audio', StandardScaler(), audio_features_columns),\n    ('text', StandardScaler(), text_features_columns)\n])\n\n# 7. Hybrid Model Architecture\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n\n# Audio branch\naudio_input = Input(shape=(audio_feature_dim,))\naudio_dense = Dense(128, activation='relu')(audio_input)\n\n# Text branch\ntext_input = Input(shape=(text_feature_dim,))\ntext_dense = Dense(128, activation='relu')(text_input)\n\n# Fusion\nmerged = Concatenate()([audio_dense, text_dense])\noutput = Dense(1, activation='linear')(merged)\n\nmodel = Model(inputs=[audio_input, text_input], outputs=output)\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n# 8. Advanced Training\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nhistory = model.fit(\n    [X_train_audio, X_train_text],\n    y_train,\n    validation_split=0.2,\n    epochs=100,\n    callbacks=[\n        EarlyStopping(patience=10),\n        ReduceLROnPlateau(factor=0.2, patience=5)\n    ]\n)\n\n# 9. Evaluation\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\ny_pred = model.predict([X_test_audio, X_test_text])\nprint(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\nprint(f\"RÂ²: {r2_score(y_test, y_pred):.2f}\")","metadata":{"_uuid":"7f869f2a-db96-432d-8ca7-5cb35b36c9b0","_cell_guid":"66584e89-8358-42dd-b128-c7d8ad962dd9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T06:08:44.685804Z","iopub.execute_input":"2025-05-04T06:08:44.686241Z","iopub.status.idle":"2025-05-04T06:10:55.442353Z","shell.execute_reply.started":"2025-05-04T06:08:44.686212Z","shell.execute_reply":"2025-05-04T06:10:55.440992Z"}},"outputs":[],"execution_count":null}]}